{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise de Filmes de arquivos .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/20 21:37:57 WARN Utils: Your hostname, Eduardos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.3.5 instead (on interface en0)\n",
      "22/10/20 21:37:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/20 21:37:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9q/4j308r1x6y36n5_m3lmd7nrr0000gn/T/ipykernel_2617/67709214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Numsey@Password!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcnxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyodbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DRIVER={ODBC Driver 17 for SQL Server};SERVER='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';DATABASE='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';UID='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';PWD='\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: ('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "import os \n",
    "import os.path as path\n",
    "import datetime as dt\n",
    "import sys\n",
    "#criando um objeto sparksession object e um appName \n",
    "spark=SparkSession.builder.master(\"local[1]\")\\\n",
    "        .appName(\"movie\")\\\n",
    "        .config(\"spark.driver.extraClassPath\",\"/Users/eduardoalberto/opt/spark/jars/mssql-jdbc-7.4.1.jre8.jar\" ) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "server = 'localhost'\n",
    "database = 'DBDWP511'\n",
    "username = 'sa'\n",
    "password = 'Numsey@Password!'\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: string (nullable = true)\n",
      " |-- crew: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 671:========>        (1 + 1) / 2][Stage 673:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+------+-----+-------------+-----+--------------------+--------------------+----------+------+-----+------+-------------+--------------------+-----+\n",
      "|cast_id|  character|           credit_id|gender| id02|         name|order|        profile_path|           credit_id|department|gender| id03|   job|         name|        profile_path|   id|\n",
      "+-------+-----------+--------------------+------+-----+-------------+-----+--------------------+--------------------+----------+------+-----+------+-------------+--------------------+-----+\n",
      "|      2|David Brent|5550bd80c3a36840a...|     2|17835|Ricky Gervais|    0|/1Mw3ZGmEh5URfjWn...|53351b4f925141444...|   Writing|     2|17835|Writer|Ricky Gervais|/1Mw3ZGmEh5URfjWn...|17835|\n",
      "+-------+-----------+--------------------+------+-----+-------------+-----+--------------------+--------------------+----------+------+-----+------+-------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/credits.csv\"\n",
    "if path.exists(file):\n",
    "# tratar do .csv\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "# cast     \n",
    "    arq.printSchema()\n",
    "    schema = StructType([\n",
    "            StructField(\"cast_id\",      StringType(),True),\n",
    "            StructField(\"character\",    StringType(),True),\n",
    "            StructField(\"credit_id\",    StringType(),True),\n",
    "            StructField(\"gender\",       StringType(),True),\n",
    "            StructField(\"id\",           StringType(),True),\n",
    "            StructField(\"name\",         StringType(),True),\n",
    "            StructField(\"order\",        StringType(),True),\n",
    "            StructField(\"profile_path\", StringType(),True)\n",
    "    ])\n",
    "    df = arq.select(regexp_replace(regexp_replace(arq.cast,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"cast\"))  \n",
    "    df1 = df.withColumn(\"cast\", from_json(\"cast\", schema)).select(col('cast.*'))\n",
    "    df1 = df1.withColumnRenamed(\"id\", \"id02\")\n",
    "# crew\n",
    "    schema2 = StructType([\n",
    "            StructField(\"credit_id\",      StringType(),True),\n",
    "            StructField(\"department\",     StringType(),True),\n",
    "            StructField(\"gender\",         StringType(),True),\n",
    "            StructField(\"id\",             StringType(),True),\n",
    "            StructField(\"job\",            StringType(),True),\n",
    "            StructField(\"name\",           StringType(),True),\n",
    "            StructField(\"profile_path\",   StringType(),True)\n",
    "    ])\n",
    "    df2=  arq.select(regexp_replace(regexp_replace(arq.crew,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"crew\"))\n",
    "    df2 = df2.withColumn(\"crew\", from_json(\"crew\", schema2)).select(col(\"crew.*\"))\n",
    "    df2 = df2.withColumnRenamed(\"id\", \"id03\")\n",
    "# ID\n",
    "    df3 = arq.select(arq.id)\n",
    "    # junta tudo\n",
    "    tbl_creditos = df1.join(df2, df1.id02 == df2.id03, \"inner\").join(df3, df3.id == df1.id02).select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+\n",
      "|id_key|   id|                name|\n",
      "+------+-----+--------------------+\n",
      "|   862|  931|            jealousy|\n",
      "|  8844|10090|          board game|\n",
      "| 15602| 1495|             fishing|\n",
      "| 31357|  818|      based on novel|\n",
      "| 11862| 1009|                baby|\n",
      "|   949|  642|             robbery|\n",
      "| 11860|   90|               paris|\n",
      "| 45325| null|                null|\n",
      "|  9091|  949|           terrorist|\n",
      "|   710|  701|                cuba|\n",
      "|  9087|  833|         white house|\n",
      "| 12110| 3633|             dracula|\n",
      "| 21032| 1994|                wolf|\n",
      "| 10858|  840|       usa president|\n",
      "|  1408|  911|       exotic island|\n",
      "|   524|  383|               poker|\n",
      "|  4584|  420|             bowling|\n",
      "|     5|  612|               hotel|\n",
      "|  9273|  409|              africa|\n",
      "| 11517|  380|brother brother r...|\n",
      "+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/keywords.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    df = spark.createDataFrame(arq.rdd)\n",
    "    sch = StructType([\n",
    "             StructField(\"id\",       IntegerType(),True),\n",
    "             StructField(\"name\",     StringType(),True)])\n",
    "    df = df.select(df.id,regexp_replace(regexp_replace(df.keywords,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"keywords\"))\n",
    "    tbl_keyworkds = df.withColumn(\"keywords\", from_json(\"keywords\", sch)).select(df.id.alias(\"id_key\"),col(\"keywords.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|imdbId|tmdbId|\n",
      "+-------+------+------+\n",
      "|      1|114709|   862|\n",
      "|      2|113497|  8844|\n",
      "|      3|113228| 15602|\n",
      "|      4|114885| 31357|\n",
      "|      5|113041| 11862|\n",
      "|      6|113277|   949|\n",
      "|      7|114319| 11860|\n",
      "|      8|112302| 45325|\n",
      "|      9|114576|  9091|\n",
      "|     10|113189|   710|\n",
      "|     11|112346|  9087|\n",
      "|     12|112896| 12110|\n",
      "|     13|112453| 21032|\n",
      "|     14|113987| 10858|\n",
      "|     15|112760|  1408|\n",
      "|     16|112641|   524|\n",
      "|     17|114388|  4584|\n",
      "|     18|113101|     5|\n",
      "|     19|112281|  9273|\n",
      "|     20|113845| 11517|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/links_small.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    tbl_links_small = arq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|imdbId|tmdbId|\n",
      "+-------+------+------+\n",
      "|      1|114709|   862|\n",
      "|      2|113497|  8844|\n",
      "|      3|113228| 15602|\n",
      "|      4|114885| 31357|\n",
      "|      5|113041| 11862|\n",
      "|      6|113277|   949|\n",
      "|      7|114319| 11860|\n",
      "|      8|112302| 45325|\n",
      "|      9|114576|  9091|\n",
      "|     10|113189|   710|\n",
      "|     11|112346|  9087|\n",
      "|     12|112896| 12110|\n",
      "|     13|112453| 21032|\n",
      "|     14|113987| 10858|\n",
      "|     15|112760|  1408|\n",
      "|     16|112641|   524|\n",
      "|     17|114388|  4584|\n",
      "|     18|113101|     5|\n",
      "|     19|112281|  9273|\n",
      "|     20|113845| 11517|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/links.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    tbl_links = arq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from email import header\n",
    "from operator import mod\n",
    "from os import sep\n",
    "\n",
    "\n",
    "file = f\"/Users/eduardoalberto/LoadFile/archive/movies_metadata.csv\"\n",
    "parquet = \"/Users/eduardoalberto/LoadFile/archive/parquet\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    # arq.printSchema()\n",
    "    movie = arq.where(arq.adult == \"False\").select(\"*\")\n",
    "    movieAdult = arq.where(arq.adult == \"True\").select(\"*\")\n",
    "    \n",
    "    sch01 = StructType([\n",
    "            StructField(\"id\",               IntegerType(),True),\n",
    "            StructField(\"name\",             StringType(),True),\n",
    "            StructField(\"poster_path\",      StringType(),True),\n",
    "            StructField(\"backdrop_path\",    StringType(),True)])\n",
    "\n",
    "    movie_01 = movie.withColumn(\"belongs_to_collection\", from_json(\"belongs_to_collection\",sch01)). \\\n",
    "          select(arq.id.alias(\"id01\"),\n",
    "                 col(\"belongs_to_collection.*\"),\n",
    "                 arq.budget,\n",
    "                 arq.imdb_id,\n",
    "                 arq.original_language,\n",
    "                 arq.original_title,\n",
    "                 arq.overview,\n",
    "                 arq.popularity,\n",
    "                 arq.poster_path.alias(\"path_poster\"),\n",
    "                 arq.release_date.cast(\"date\").alias(\"dt_release\"),\n",
    "                 arq.revenue,\n",
    "                 arq.runtime,\n",
    "                 arq.status,\n",
    "                 arq.tagline,\n",
    "                 arq.title,\n",
    "                 arq.video,\n",
    "                 arq.vote_average.cast(\"int\").alias(\"vote_average\"),\n",
    "                 arq.vote_count.cast(\"int\").alias(\"vote_count\")\n",
    "                 ) \n",
    " \n",
    "    sch02 = StructType([\n",
    "            StructField(\"id\",               IntegerType(),True),\n",
    "            StructField(\"name\",             StringType(),True)])\n",
    "    \n",
    "  # movie.select(\"genres\").show(10, False)\n",
    "    mv = movie.select(regexp_replace(regexp_replace(movie.genres,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"genres\"))\n",
    "    mv = mv.withColumn(\"genres\", from_json(\"genres\",sch02)).select(f.col(\"genres.*\"))\n",
    "    mv = mv.withColumnRenamed(\"id\", \"id02\")\n",
    "    mv = mv.withColumnRenamed(\"name\", \"name_geners\")\n",
    "    df_full = movie_01.join(mv, movie_01.id01 == mv.id02, \"inner\")\n",
    "\n",
    "    df_full.write.parquet(path=parquet, mode=\"overwrite\")\n",
    "\n",
    "   #usando coalesce pra gerar arquivo unico\n",
    "    df_full.coalesce(1).write.csv(path=parquet, mode=\"overwrite\",sep=\";\",header=True)\n",
    "\n",
    "    \n",
    "\n",
    "    spark.stop\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2562\n",
      "+------------------------------+-----+\n",
      "|nome                          |idade|\n",
      "+------------------------------+-----+\n",
      "|GISELLE PAULA GUIMARAES CASTRO|15   |\n",
      "|ELAINE GARCIA DE OLIVEIRA     |22   |\n",
      "|JOAO CARLOS ABNER DE LOURDES  |43   |\n",
      "|MARTA ZELI FERREIRA           |24   |\n",
      "|LAUDENETE WIGGERS ROEDER      |51   |\n",
      "+------------------------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|ident            |idade|\n",
      "+-----------------+-----+\n",
      "|GISELLE, CASTRO  |15   |\n",
      "|ELAINE, OLIVEIRA |22   |\n",
      "|JOAO, LOURDES    |43   |\n",
      "|MARTA, FERREIRA  |24   |\n",
      "|LAUDENETE, ROEDER|51   |\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "porcent = lambda nulos, massa: (nulos / massa)*100\n",
    "print((porcent(182562,1000000)))\n",
    "\n",
    "data = [\n",
    "    ('GISELLE PAULA GUIMARAES CASTRO', 15),\n",
    "    ('ELAINE GARCIA DE OLIVEIRA', 22),\n",
    "    ('JOAO CARLOS ABNER DE LOURDES', 43),\n",
    "    ('MARTA ZELI FERREIRA', 24),\n",
    "    ('LAUDENETE WIGGERS ROEDER', 51)\n",
    "]\n",
    "colNames = ['nome', 'idade']\n",
    "df = spark.createDataFrame(data, colNames)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df \\\n",
    "    .select(\n",
    "        f.concat_ws(\n",
    "            ', ', \n",
    "            f.substring_index('nome', ' ', 1), \n",
    "            f.substring_index('nome', ' ', -1)\n",
    "        ).alias('ident'), \n",
    "        'idade') \\\n",
    "    .show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
