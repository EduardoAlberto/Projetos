{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise de Filmes de arquivos .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "import os \n",
    "import os.path as path\n",
    "import datetime as dt\n",
    "import sys\n",
    "#criando um objeto sparksession object e um appName \n",
    "spark=SparkSession.builder.master(\"local[1]\")\\\n",
    "        .appName(\"movie\")\\\n",
    "        .config(\"spark.driver.extraClassPath\",\"/Users/eduardoalberto/opt/spark/jars/mssql-jdbc-7.4.1.jre8.jar\" ) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "server = 'localhost'\n",
    "database = 'DBDWP511'\n",
    "username = 'sa'\n",
    "password = 'Numsey@Password!'\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/credits.csv\"\n",
    "if path.exists(file):\n",
    "# tratar do .csv\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "# cast     \n",
    "    arq.printSchema()\n",
    "    schema = StructType([\n",
    "            StructField(\"cast_id\",      StringType(),True),\n",
    "            StructField(\"character\",    StringType(),True),\n",
    "            StructField(\"credit_id\",    StringType(),True),\n",
    "            StructField(\"gender\",       StringType(),True),\n",
    "            StructField(\"id\",           StringType(),True),\n",
    "            StructField(\"name\",         StringType(),True),\n",
    "            StructField(\"order\",        StringType(),True),\n",
    "            StructField(\"profile_path\", StringType(),True)\n",
    "    ])\n",
    "    df = arq.select(regexp_replace(regexp_replace(arq.cast,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"cast\"))  \n",
    "    df1 = df.withColumn(\"cast\", from_json(\"cast\", schema)).select(col('cast.*'))\n",
    "    df1 = df1.withColumnRenamed(\"id\", \"id02\")\n",
    "# crew\n",
    "    schema2 = StructType([\n",
    "            StructField(\"credit_id\",      StringType(),True),\n",
    "            StructField(\"department\",     StringType(),True),\n",
    "            StructField(\"gender\",         StringType(),True),\n",
    "            StructField(\"id\",             StringType(),True),\n",
    "            StructField(\"job\",            StringType(),True),\n",
    "            StructField(\"name\",           StringType(),True),\n",
    "            StructField(\"profile_path\",   StringType(),True)\n",
    "    ])\n",
    "    df2=  arq.select(regexp_replace(regexp_replace(arq.crew,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"crew\"))\n",
    "    df2 = df2.withColumn(\"crew\", from_json(\"crew\", schema2)).select(col(\"crew.*\"))\n",
    "    df2 = df2.withColumnRenamed(\"id\", \"id03\")\n",
    "# ID\n",
    "    df3 = arq.select(arq.id)\n",
    "    # junta tudo\n",
    "    tbl_creditos = df1.join(df2, df1.id02 == df2.id03, \"inner\").join(df3, df3.id == df1.id02).select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/keywords.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    df = spark.createDataFrame(arq.rdd)\n",
    "    sch = StructType([\n",
    "             StructField(\"id\",       IntegerType(),True),\n",
    "             StructField(\"name\",     StringType(),True)])\n",
    "    df = df.select(df.id,regexp_replace(regexp_replace(df.keywords,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"keywords\"))\n",
    "    tbl_keyworkds = df.withColumn(\"keywords\", from_json(\"keywords\", sch)).select(df.id.alias(\"id_key\"),col(\"keywords.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/links_small.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    tbl_links_small = arq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"/Users/eduardoalberto/LoadFile/archive/links.csv\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    tbl_links = arq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import header\n",
    "from operator import mod\n",
    "from os import sep\n",
    "\n",
    "\n",
    "file = f\"/Users/eduardoalberto/LoadFile/archive/movies_metadata.csv\"\n",
    "parquet = \"/Users/eduardoalberto/LoadFile/archive/parquet\"\n",
    "if path.exists(file):\n",
    "    arq = spark.read.option(\"delimiter\", \",\").option(\"header\", True).option(\"inferSchema\", True).option(\"escape\",'\"').csv(file)\n",
    "    # arq.printSchema()\n",
    "    movie = arq.where(arq.adult == \"False\").select(\"*\")\n",
    "    movieAdult = arq.where(arq.adult == \"True\").select(\"*\")\n",
    "    \n",
    "    sch01 = StructType([\n",
    "            StructField(\"id\",               IntegerType(),True),\n",
    "            StructField(\"name\",             StringType(),True),\n",
    "            StructField(\"poster_path\",      StringType(),True),\n",
    "            StructField(\"backdrop_path\",    StringType(),True)])\n",
    "\n",
    "    movie_01 = movie.withColumn(\"belongs_to_collection\", from_json(\"belongs_to_collection\",sch01)). \\\n",
    "          select(arq.id.alias(\"id01\"),\n",
    "                 col(\"belongs_to_collection.*\"),\n",
    "                 arq.budget,\n",
    "                 arq.imdb_id,\n",
    "                 arq.original_language,\n",
    "                 arq.original_title,\n",
    "                 arq.overview,\n",
    "                 arq.popularity,\n",
    "                 arq.poster_path.alias(\"path_poster\"),\n",
    "                 arq.release_date.cast(\"date\").alias(\"dt_release\"),\n",
    "                 arq.revenue,\n",
    "                 arq.runtime,\n",
    "                 arq.status,\n",
    "                 arq.tagline,\n",
    "                 arq.title,\n",
    "                 arq.video,\n",
    "                 arq.vote_average.cast(\"int\").alias(\"vote_average\"),\n",
    "                 arq.vote_count.cast(\"int\").alias(\"vote_count\")\n",
    "                 ) \n",
    " \n",
    "    sch02 = StructType([\n",
    "            StructField(\"id\",               IntegerType(),True),\n",
    "            StructField(\"name\",             StringType(),True)])\n",
    "    \n",
    "  # movie.select(\"genres\").show(10, False)\n",
    "    mv = movie.select(regexp_replace(regexp_replace(movie.genres,r\"\\[\",\"\"),r\"\\]\",\"\").alias(\"genres\"))\n",
    "    mv = mv.withColumn(\"genres\", from_json(\"genres\",sch02)).select(f.col(\"genres.*\"))\n",
    "    mv = mv.withColumnRenamed(\"id\", \"id02\")\n",
    "    mv = mv.withColumnRenamed(\"name\", \"name_geners\")\n",
    "    df_full = movie_01.join(mv, movie_01.id01 == mv.id02, \"inner\")\n",
    "\n",
    "    df_full.write.parquet(path=parquet, mode=\"overwrite\")\n",
    "\n",
    "   #usando coalesce pra gerar arquivo unico\n",
    "    df_full.coalesce(1).write.csv(path=parquet, mode=\"overwrite\",sep=\";\",header=True)\n",
    "\n",
    "    \n",
    "\n",
    "    spark.stop\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porcent = lambda nulos, massa: (nulos / massa)*100\n",
    "print((porcent(182562,1000000)))\n",
    "\n",
    "data = [\n",
    "    ('GISELLE PAULA GUIMARAES CASTRO', 15),\n",
    "    ('ELAINE GARCIA DE OLIVEIRA', 22),\n",
    "    ('JOAO CARLOS ABNER DE LOURDES', 43),\n",
    "    ('MARTA ZELI FERREIRA', 24),\n",
    "    ('LAUDENETE WIGGERS ROEDER', 51)\n",
    "]\n",
    "colNames = ['nome', 'idade']\n",
    "df = spark.createDataFrame(data, colNames)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df \\\n",
    "    .select(\n",
    "        f.concat_ws(\n",
    "            ', ', \n",
    "            f.substring_index('nome', ' ', 1), \n",
    "            f.substring_index('nome', ' ', -1)\n",
    "        ).alias('ident'), \n",
    "        'idade') \\\n",
    "    .show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
