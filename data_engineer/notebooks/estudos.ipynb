{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/05 22:30:58 WARN Utils: Your hostname, Eduardos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.3.5 instead (on interface en0)\n",
      "23/05/05 22:30:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/05 22:30:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/05 22:31:15 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9q/4j308r1x6y36n5_m3lmd7nrr0000gn/T/ipykernel_77516/416777123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Numsey@Password!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcnxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyodbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DRIVER={ODBC Driver 17 for SQL Server};SERVER='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';DATABASE='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';UID='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m';PWD='\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: ('HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "import os \n",
    "import os.path as path\n",
    "import datetime as dt\n",
    "import sys\n",
    "#criando um objeto sparksession object e um appName \n",
    "spark=SparkSession.builder.master(\"local[1]\")\\\n",
    "        .appName(\"movie\")\\\n",
    "        .config(\"spark.driver.extraClassPath\",\"/Users/eduardoalberto/opt/spark/jars/mssql-jdbc-7.4.1.jre8.jar\" ) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "\n",
    "server = 'localhost'\n",
    "database = 'DBDWP511'\n",
    "username = 'sa'\n",
    "password = 'Numsey@Password!'\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usando python puro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsql = \"select * from DimDate;\"\n",
    "with cursor.execute(tsql):\n",
    "    for row in cursor:\n",
    "        print(str(row[0:]))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usando pandas para conectar ao banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateKey</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Title</th>\n",
       "      <th>EmployeeKey</th>\n",
       "      <th>ParentEmployeeKey</th>\n",
       "      <th>EmployeeNationalIDAlternateKey</th>\n",
       "      <th>ParentEmployeeNationalIDAlternateKey</th>\n",
       "      <th>SalesTerritoryKey</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>DayNumberOfWeek</th>\n",
       "      <th>EnglishMonthName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20060128</td>\n",
       "      <td>Guy</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14417807</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20060826</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>253022876</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20070611</td>\n",
       "      <td>Roberto</td>\n",
       "      <td>Tamburello</td>\n",
       "      <td>Engineering Manager</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>509647174</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>June</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20070705</td>\n",
       "      <td>Rob</td>\n",
       "      <td>Walters</td>\n",
       "      <td>Senior Tool Designer</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112457891</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>July</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091228</td>\n",
       "      <td>Rob</td>\n",
       "      <td>Walters</td>\n",
       "      <td>Senior Tool Designer</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112457891</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20111229</td>\n",
       "      <td>Ranjit</td>\n",
       "      <td>Varkey Chudukatil</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>292</td>\n",
       "      <td>290.0</td>\n",
       "      <td>134219713</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20120430</td>\n",
       "      <td>Tete</td>\n",
       "      <td>Mensa-Annan</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>293</td>\n",
       "      <td>272.0</td>\n",
       "      <td>90836195</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20121012</td>\n",
       "      <td>Syed</td>\n",
       "      <td>Abbas</td>\n",
       "      <td>Pacific Sales Manager</td>\n",
       "      <td>294</td>\n",
       "      <td>277.0</td>\n",
       "      <td>481044938</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>October</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20121228</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>Valdez</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>295</td>\n",
       "      <td>290.0</td>\n",
       "      <td>954276278</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>20121228</td>\n",
       "      <td>Lynn</td>\n",
       "      <td>Tsoflias</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>296</td>\n",
       "      <td>294.0</td>\n",
       "      <td>758596752</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DateKey FirstName           LastName                         Title  \\\n",
       "0    20060128       Guy            Gilbert  Production Technician - WC60   \n",
       "1    20060826     Kevin              Brown           Marketing Assistant   \n",
       "2    20070611   Roberto         Tamburello           Engineering Manager   \n",
       "3    20070705       Rob            Walters          Senior Tool Designer   \n",
       "4    20091228       Rob            Walters          Senior Tool Designer   \n",
       "..        ...       ...                ...                           ...   \n",
       "291  20111229    Ranjit  Varkey Chudukatil          Sales Representative   \n",
       "292  20120430      Tete        Mensa-Annan          Sales Representative   \n",
       "293  20121012      Syed              Abbas         Pacific Sales Manager   \n",
       "294  20121228    Rachel             Valdez          Sales Representative   \n",
       "295  20121228      Lynn           Tsoflias          Sales Representative   \n",
       "\n",
       "     EmployeeKey  ParentEmployeeKey EmployeeNationalIDAlternateKey  \\\n",
       "0              1               18.0                       14417807   \n",
       "1              2                7.0                      253022876   \n",
       "2              3               14.0                      509647174   \n",
       "3              4                3.0                      112457891   \n",
       "4              5                3.0                      112457891   \n",
       "..           ...                ...                            ...   \n",
       "291          292              290.0                      134219713   \n",
       "292          293              272.0                       90836195   \n",
       "293          294              277.0                      481044938   \n",
       "294          295              290.0                      954276278   \n",
       "295          296              294.0                      758596752   \n",
       "\n",
       "    ParentEmployeeNationalIDAlternateKey  SalesTerritoryKey  SalariedFlag  \\\n",
       "0                                   None                 11         False   \n",
       "1                                   None                 11         False   \n",
       "2                                   None                 11          True   \n",
       "3                                   None                 11         False   \n",
       "4                                   None                 11         False   \n",
       "..                                   ...                ...           ...   \n",
       "291                                 None                  7          True   \n",
       "292                                 None                  1          True   \n",
       "293                                 None                 11          True   \n",
       "294                                 None                  8          True   \n",
       "295                                 None                  9          True   \n",
       "\n",
       "     DayNumberOfWeek EnglishMonthName  \n",
       "0                  7          January  \n",
       "1                  7           August  \n",
       "2                  2             June  \n",
       "3                  5             July  \n",
       "4                  2         December  \n",
       "..               ...              ...  \n",
       "291                5         December  \n",
       "292                2            April  \n",
       "293                6          October  \n",
       "294                6         December  \n",
       "295                6         December  \n",
       "\n",
       "[296 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tables = pd.read_sql_query(\"select * from INFORMATION_SCHEMA.TABLES;\",cnxn)\n",
    "DimEmployee = pd.read_sql_query(\"select * from DimEmployee;\",cnxn)\n",
    "DimDate = pd.read_sql_query(\"select * from DimDate;\",cnxn)\n",
    "\n",
    "DimEmployee[\"DateKey\"] = pd.to_datetime(DimEmployee['StartDate']).dt.strftime('%Y%m%d')\n",
    "DimEmployee[\"DateKey\"] = pd.to_numeric(DimEmployee[\"DateKey\"])\n",
    "\n",
    "\n",
    "# t1 =DimEmployee[(DimEmployee[\"DateKey\"] >= \"20050101\")]\n",
    "t1 =DimEmployee[[\"DateKey\",\"FirstName\",\"LastName\",\"Title\",\"EmployeeKey\",\"ParentEmployeeKey\",\"EmployeeNationalIDAlternateKey\",\"ParentEmployeeNationalIDAlternateKey\",\"SalesTerritoryKey\",\"SalariedFlag\"]]\n",
    "t2 =DimDate[[\"DateKey\",\"DayNumberOfWeek\",\"EnglishMonthName\"]]\n",
    "\n",
    "# display(t1.dtypes)\n",
    "# display(t2.dtypes)\n",
    "\n",
    "t3 = pd.merge(t1, t2, on=\"DateKey\")\n",
    "# t3 = t1.join(t2, lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "display(t3)\n",
    "# t3.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerKey', 'int'),\n",
       " ('GeographyKey', 'int'),\n",
       " ('CustomerAlternateKey', 'string'),\n",
       " ('Title', 'string'),\n",
       " ('FirstName', 'string'),\n",
       " ('MiddleName', 'string'),\n",
       " ('LastName', 'string'),\n",
       " ('NameStyle', 'boolean'),\n",
       " ('BirthDate', 'date'),\n",
       " ('MaritalStatus', 'string'),\n",
       " ('Suffix', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('EmailAddress', 'string'),\n",
       " ('YearlyIncome', 'decimal(19,4)'),\n",
       " ('TotalChildren', 'int'),\n",
       " ('NumberChildrenAtHome', 'int'),\n",
       " ('EnglishEducation', 'string'),\n",
       " ('SpanishEducation', 'string'),\n",
       " ('FrenchEducation', 'string'),\n",
       " ('EnglishOccupation', 'string'),\n",
       " ('SpanishOccupation', 'string'),\n",
       " ('FrenchOccupation', 'string'),\n",
       " ('HouseOwnerFlag', 'string'),\n",
       " ('NumberCarsOwned', 'int'),\n",
       " ('AddressLine1', 'string'),\n",
       " ('AddressLine2', 'string'),\n",
       " ('Phone', 'string'),\n",
       " ('DateFirstPurchase', 'date'),\n",
       " ('CommuteDistance', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('SurveyResponseKey', 'int'),\n",
       " ('DateKey', 'int'),\n",
       " ('CustomerKey', 'int'),\n",
       " ('ProductCategoryKey', 'int'),\n",
       " ('EnglishProductCategoryName', 'string'),\n",
       " ('ProductSubcategoryKey', 'int'),\n",
       " ('EnglishProductSubcategoryName', 'string'),\n",
       " ('Date', 'timestamp')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"DimCustomer\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "t3 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"FactSurveyResponse\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "\n",
    "display(t2.dtypes)\n",
    "display(t3.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerKey', 'int'),\n",
       " ('GeographyKey', 'int'),\n",
       " ('CustomerAlternateKey', 'string'),\n",
       " ('Title', 'string'),\n",
       " ('FirstName', 'string'),\n",
       " ('MiddleName', 'string'),\n",
       " ('LastName', 'string'),\n",
       " ('NameStyle', 'boolean'),\n",
       " ('BirthDate', 'date'),\n",
       " ('MaritalStatus', 'string'),\n",
       " ('Suffix', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('EmailAddress', 'string'),\n",
       " ('YearlyIncome', 'decimal(19,4)'),\n",
       " ('TotalChildren', 'int'),\n",
       " ('NumberChildrenAtHome', 'int'),\n",
       " ('EnglishEducation', 'string'),\n",
       " ('SpanishEducation', 'string'),\n",
       " ('FrenchEducation', 'string'),\n",
       " ('EnglishOccupation', 'string'),\n",
       " ('SpanishOccupation', 'string'),\n",
       " ('FrenchOccupation', 'string'),\n",
       " ('HouseOwnerFlag', 'string'),\n",
       " ('NumberCarsOwned', 'int'),\n",
       " ('AddressLine1', 'string'),\n",
       " ('AddressLine2', 'string'),\n",
       " ('Phone', 'string'),\n",
       " ('DateFirstPurchase', 'date'),\n",
       " ('CommuteDistance', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('SurveyResponseKey', 'int'),\n",
       " ('DateKey', 'int'),\n",
       " ('CustomerKey', 'int'),\n",
       " ('ProductCategoryKey', 'int'),\n",
       " ('EnglishProductCategoryName', 'string'),\n",
       " ('ProductSubcategoryKey', 'int'),\n",
       " ('EnglishProductSubcategoryName', 'string'),\n",
       " ('Date', 'timestamp')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"DimCustomer\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "t3 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"FactSurveyResponse\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "\n",
    "display(t2.dtypes)\n",
    "display(t3.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerKey', 'int'),\n",
       " ('GeographyKey', 'int'),\n",
       " ('CustomerAlternateKey', 'string'),\n",
       " ('Title', 'string'),\n",
       " ('FirstName', 'string'),\n",
       " ('MiddleName', 'string'),\n",
       " ('LastName', 'string'),\n",
       " ('NameStyle', 'boolean'),\n",
       " ('BirthDate', 'date'),\n",
       " ('MaritalStatus', 'string'),\n",
       " ('Suffix', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('EmailAddress', 'string'),\n",
       " ('YearlyIncome', 'decimal(19,4)'),\n",
       " ('TotalChildren', 'int'),\n",
       " ('NumberChildrenAtHome', 'int'),\n",
       " ('EnglishEducation', 'string'),\n",
       " ('SpanishEducation', 'string'),\n",
       " ('FrenchEducation', 'string'),\n",
       " ('EnglishOccupation', 'string'),\n",
       " ('SpanishOccupation', 'string'),\n",
       " ('FrenchOccupation', 'string'),\n",
       " ('HouseOwnerFlag', 'string'),\n",
       " ('NumberCarsOwned', 'int'),\n",
       " ('AddressLine1', 'string'),\n",
       " ('AddressLine2', 'string'),\n",
       " ('Phone', 'string'),\n",
       " ('DateFirstPurchase', 'date'),\n",
       " ('CommuteDistance', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('SurveyResponseKey', 'int'),\n",
       " ('DateKey', 'int'),\n",
       " ('CustomerKey', 'int'),\n",
       " ('ProductCategoryKey', 'int'),\n",
       " ('EnglishProductCategoryName', 'string'),\n",
       " ('ProductSubcategoryKey', 'int'),\n",
       " ('EnglishProductSubcategoryName', 'string'),\n",
       " ('Date', 'timestamp')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"DimCustomer\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "t3 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"FactSurveyResponse\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "\n",
    "display(t2.dtypes)\n",
    "display(t3.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|CustomerKey| DateKey|\n",
      "+-----------+--------+\n",
      "|      11033|20120310|\n",
      "|      11033|20120310|\n",
      "|      11033|20120310|\n",
      "|      16339|20120329|\n",
      "|      17753|20120318|\n",
      "|      18944|20120325|\n",
      "|      23271|20120324|\n",
      "|      28759|20120427|\n",
      "|      28759|20120427|\n",
      "|      28836|20120229|\n",
      "|      28836|20120229|\n",
      "|      19204|20120328|\n",
      "|      18800|20120318|\n",
      "|      11141|20120417|\n",
      "|      20497|20120301|\n",
      "|      18800|20120318|\n",
      "|      11141|20120417|\n",
      "|      20497|20120301|\n",
      "|      11316|20120325|\n",
      "|      11316|20120325|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2.join(t3, t2.CustomerKey == t3.CustomerKey, \"inner\").select(t2.CustomerKey,t3.DateKey).show()\n",
    "# t4.dtypes\n",
    "# t4.select(\n",
    "#      \"GeographyKey\"\n",
    "#     ,\"CustomerAlternateKey\"\n",
    "#     ,\"Title\"\n",
    "#     ,\"FirstName\"\n",
    "#     ,\"MiddleName\"\n",
    "#     ,\"LastName\"\n",
    "#     ,\"NameStyle\"\n",
    "#     ,\"BirthDate\"\n",
    "#     ,\"MaritalStatus\"\n",
    "#     ,\"Suffix\"\n",
    "#     ,\"Gender\"\n",
    "#     ,\"EmailAddress\"\n",
    "#     ,\"YearlyIncome\"\n",
    "#     ,\"TotalChildren\"\n",
    "#     ,\"NumberChildrenAtHome\"\n",
    "#     ,\"EnglishEducation\"\n",
    "#     ,\"HouseOwnerFlag\"\n",
    "#     ,\"NumberCarsOwned\"\n",
    "#     ,\"AddressLine1\"\n",
    "#     ,\"ProductCategoryKey\"\n",
    "#     ,\"EnglishProductCategoryName\"\n",
    "#     ,\"ProductSubcategoryKey\"\n",
    "#     ,\"EnglishProductSubcategoryName\"\n",
    "#     ,\"Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|json                                                                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"id\":12345,\"foo\":\"bar\",\"bodyid\":111000,\"name\":\"foobar\",\"sub_jsonid\":54321,\"sub_sub_jsoncol1\":20,\"col2\":\"somethong\"}      |\n",
      "|{\"id\":12346,\"foo\":\"baz\",\"bodyid\":111002,\"name\":\"barfoo\",\"sub_jsonid\":23456,\"sub_sub_jsoncol1\":30,\"col2\":\"something else\"} |\n",
      "|{\"id\":43256,\"foo\":\"foobaz\",\"bodyid\":20192,\"name\":\"bazbar\",\"sub_jsonid\":39283,\"sub_sub_jsoncol1\":50,\"col2\":\"another thing\"}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "jstr1 = u'{\"id\":12345,\"foo\":\"bar\",\"bodyid\":111000,\"name\":\"foobar\",\"sub_jsonid\":54321,\"sub_sub_jsoncol1\":20,\"col2\":\"somethong\"}'\n",
    "jstr1 = u'{\"id\":12345,\"foo\":\"bar\",\"bodyid\":111000,\"name\":\"foobar\",\"sub_jsonid\":54321,\"sub_sub_jsoncol1\":20,\"col2\":\"somethong\"}'\n",
    "jstr2 = u'{\"id\":12346,\"foo\":\"baz\",\"bodyid\":111002,\"name\":\"barfoo\",\"sub_jsonid\":23456,\"sub_sub_jsoncol1\":30,\"col2\":\"something else\"}'\n",
    "jstr3 = u'{\"id\":43256,\"foo\":\"foobaz\",\"bodyid\":20192,\"name\":\"bazbar\",\"sub_jsonid\":39283,\"sub_sub_jsoncol1\":50,\"col2\":\"another thing\"}'\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "\n",
    "df01 = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)])\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "\n",
    "df01.show(truncate=False)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df02 = df01.select(df01.json, json_tuple(df01.json,\"id\", \"foo\", \"bodyid\",\"name\",\"foobar\",\"sub_jsonid\",\"sub_sub_jsoncol1\",\"col2\")) \\\n",
    "    .toDF(\"json\", \"id\", \"foo\", \"bodyid\",\"name\",\"foobar\",\"sub_jsonid\",\"sub_sub_jsoncol1\",\"col2\") \\\n",
    "    # .show(truncate=False)\n",
    "\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\",\"State\")) \\\n",
    ".toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\",\"State\") \\\n",
    ".show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "df02.createOrReplaceTempView(\"tempAnalise\")\n",
    "\n",
    "spark.sql(\"select id, foo, bodyid,name,foobar,sub_jsonid,sub_sub_jsoncol1,col2 from tempAnalise\").show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|City               |RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|2           |PR   |STANDARD   |704    |\n",
      "|BDA SAN LUIS       |10          |PR   |STANDARD   |709    |\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base= [{\n",
    "  \"RecordNumber\": 2,\n",
    "  \"Zipcode\": 704,\n",
    "  \"ZipCodeType\": \"STANDARD\",\n",
    "  \"City\": \"PASEO COSTA DEL SUR\",\n",
    "  \"State\": \"PR\"\n",
    "},\n",
    "{\n",
    "  \"RecordNumber\": 10,\n",
    "  \"Zipcode\": 709,\n",
    "  \"ZipCodeType\": \"STANDARD\",\n",
    "  \"City\": \"BDA SAN LUIS\",\n",
    "  \"State\": \"PR\"\n",
    "}]\n",
    "\n",
    "df02 = spark.createDataFrame(base)\n",
    "df02.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 111000, 'name': 'foobar', 'sub_json': {'id': 54321, 'sub_sub_json': {'col1': 20, 'col2': 'somethong'}}}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "import json\n",
    "\n",
    "\n",
    "jstr1 = u'{\"header\":{\"id\":12345,\"foo\":\"bar\"},\"body\":{\"id\":111000,\"name\":\"foobar\",\"sub_json\":{\"id\":54321,\"sub_sub_json\":{\"col1\":20,\"col2\":\"somethong\"}}}}'\n",
    "jstr2 = u'{\"header\":{\"id\":12346,\"foo\":\"baz\"},\"body\":{\"id\":111002,\"name\":\"barfoo\",\"sub_json\":{\"id\":23456,\"sub_sub_json\":{\"col1\":30,\"col2\":\"something else\"}}}}'\n",
    "jstr3 = u'{\"header\":{\"id\":43256,\"foo\":\"foobaz\"},\"body\":{\"id\":20192,\"name\":\"bazbar\",\"sub_json\":{\"id\":39283,\"sub_sub_json\":{\"col1\":50,\"col2\":\"another thing\"}}}}'\n",
    "# schema = StructType([StructField('json', StringType(), True)])\n",
    "# cria Dataframe\n",
    "df0 = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)],schema)\n",
    "# new_df = spark.read.json(df.rdd.map(lambda r: r.json))\n",
    "# new_df.printSchema()\n",
    "\n",
    "data = json.loads(jstr1)\n",
    "print(data.get(\"body\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\n",
      "|body                                                                                                                                                                                                             |id |title                                                                     |userId|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\n",
      "|quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto                                                |1  |sunt aut facere repellat provident occaecati excepturi optio reprehenderit|1     |\n",
      "|est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla|2  |qui est esse                                                              |1     |\n",
      "|et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut                                          |3  |ea molestias quasi exercitationem repellat qui ipsa sit aut               |1     |\n",
      "|ullam et saepe reiciendis voluptatem adipisci\\nsit amet autem assumenda provident rerum culpa\\nquis hic commodi nesciunt rem tenetur doloremque ipsam iure\\nquis sunt voluptatem rerum illo velit                |4  |eum et est occaecati                                                      |1     |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+--------------------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummyDB01 = [\n",
    "   {\n",
    "      \"userId\":1,\n",
    "      \"id\":1,\n",
    "      \"title\":\"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\",\n",
    "      \"body\":\"quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\"\n",
    "   },\n",
    "   {\n",
    "      \"userId\":1,\n",
    "      \"id\":2,\n",
    "      \"title\":\"qui est esse\",\n",
    "      \"body\":\"est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla\"\n",
    "   },\n",
    "   {\n",
    "      \"userId\":1,\n",
    "      \"id\":3,\n",
    "      \"title\":\"ea molestias quasi exercitationem repellat qui ipsa sit aut\",\n",
    "      \"body\":\"et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut\"\n",
    "   },\n",
    "   {\n",
    "      \"userId\":1,\n",
    "      \"id\":4,\n",
    "      \"title\":\"eum et est occaecati\",\n",
    "      \"body\":\"ullam et saepe reiciendis voluptatem adipisci\\nsit amet autem assumenda provident rerum culpa\\nquis hic commodi nesciunt rem tenetur doloremque ipsam iure\\nquis sunt voluptatem rerum illo velit\"\n",
    "   }\n",
    "   \n",
    "   \n",
    "]\n",
    "\n",
    "df002 = spark.createDataFrame(dummyDB01)\n",
    "df002.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|email            |\n",
      "+-----------------+\n",
      "|Sincere@april.biz|\n",
      "|Shanna@melissa.tv|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummyDB02 = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"name\":\"Leanne Graham\",\n",
    "      \"username\":\"Bret\",\n",
    "      \"email\":\"Sincere@april.biz\",\n",
    "      \"address\":{\n",
    "         \"street\":\"Kulas Light\",\n",
    "         \"suite\":\"Apt. 556\",\n",
    "         \"city\":\"Gwenborough\",\n",
    "         \"zipcode\":\"92998-3874\",\n",
    "         \"geo\":{\n",
    "            \"lat\":\"-37.3159\",\n",
    "            \"lng\":\"81.1496\"\n",
    "         }\n",
    "      },\n",
    "      \"phone\":\"1-770-736-8031 x56442\",\n",
    "      \"website\":\"hildegard.org\",\n",
    "      \"company\":{\n",
    "         \"name\":\"Romaguera-Crona\",\n",
    "         \"catchPhrase\":\"Multi-layered client-server neural-net\",\n",
    "         \"bs\":\"harness real-time e-markets\"\n",
    "      }\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"name\":\"Ervin Howell\",\n",
    "      \"username\":\"Antonette\",\n",
    "      \"email\":\"Shanna@melissa.tv\",\n",
    "      \"address\":{\n",
    "         \"street\":\"Victor Plains\",\n",
    "         \"suite\":\"Suite 879\",\n",
    "         \"city\":\"Wisokyburgh\",\n",
    "         \"zipcode\":\"90566-7771\",\n",
    "         \"geo\":{\n",
    "            \"lat\":\"-43.9509\",\n",
    "            \"lng\":\"-34.4618\"\n",
    "         }\n",
    "      },\n",
    "      \"phone\":\"010-692-6593 x09125\",\n",
    "      \"website\":\"anastasia.net\",\n",
    "      \"company\":{\n",
    "         \"name\":\"Deckow-Crist\",\n",
    "         \"catchPhrase\":\"Proactive didactic contingency\",\n",
    "         \"bs\":\"synergize scalable supply-chains\"\n",
    "      }\n",
    "   }\n",
    "   \n",
    "]\n",
    "df003 = spark.createDataFrame(dummyDB02)\n",
    "\n",
    "df003 = df003.select(df003.email).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+-----------------+---+-------------+---------------------+---------+-------------+\n",
    "|address                                                                                                                       |company                                                                                                            |email            |id |name         |phone                |username |website      |\n",
    "+------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+-----------------+---+-------------+---------------------+---------+-------------+\n",
    "|{geo -> {lng=81.1496, lat=-37.3159}, zipcode -> 92998-3874, suite -> Apt. 556, city -> Gwenborough, street -> Kulas Light}    |{name -> Romaguera-Crona, bs -> harness real-time e-markets, catchPhrase -> Multi-layered client-server neural-net}|Sincere@april.biz|1  |Leanne Graham|1-770-736-8031 x56442|Bret     |hildegard.org|\n",
    "|{geo -> {lng=-34.4618, lat=-43.9509}, zipcode -> 90566-7771, suite -> Suite 879, city -> Wisokyburgh, street -> Victor Plains}|{name -> Deckow-Crist, bs -> synergize scalable supply-chains, catchPhrase -> Proactive didactic contingency}      |Shanna@melissa.tv|2  |Ervin Howell |010-692-6593 x09125  |Antonette|anastasia.net|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estudo sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|YEAR|RANK|\n",
      "+----+----+\n",
      "|1971|   0|\n",
      "|1976|   0|\n",
      "|1971|   0|\n",
      "|1973|   0|\n",
      "|1979|   0|\n",
      "|1976|   0|\n",
      "|1976|   0|\n",
      "|1969|   0|\n",
      "|1975|   0|\n",
      "|1969|   0|\n",
      "|1969|   0|\n",
      "|1969|   0|\n",
      "|1979|   0|\n",
      "|1979|   0|\n",
      "|1973|   0|\n",
      "|1984|   1|\n",
      "|1984|   1|\n",
      "|1949|   0|\n",
      "|1955|   0|\n",
      "|1983|   1|\n",
      "+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2 =  t2.withColumn(\"YEAR\",(substring(\"BirthDate\",1,4)))\n",
    "t2 =  t2.withColumn(\"RANK\", when((col(\"YEAR\") >= \"1982\") & (col(\"YEAR\") <= \"1990\") ,lit(1)).otherwise(0)) #.select(col(\"RANK\")).show()\n",
    "# df.withColumn(my_column, when((col(my_column) < '1900-01-01') | (col(my_column) > '2019-12-09 17:01:37.774418'), lit(None)).otherwise(col(my_column)))\n",
    "# t2.printSchema()\n",
    "t2.select(t2.YEAR, t2.RANK).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"DimCustomer\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})\n",
    "t3 = spark.read.jdbc(\"jdbc:sqlserver://localhost:1433;databaseName=DBDWP511\", \"FactSurveyResponse\",properties={\"user\": \"sa\", \"password\": \"Numsey@Password!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "print(p1.name)\n",
    "print(p1.age)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de classe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------+---------+--------------+-----------+-----------+-----------+-----------+----------+----------+-------------------+----------+-------+-------+---------+---------+-------------+\n",
      "|    tournament|               home|home_goal|away_goal|          away|home_corner|away_corner|home_attack|away_attack|home_shots|away_shots|               time|      date|ht_diff|at_diff|ht_result|at_result|total_corners|\n",
      "+--------------+-------------------+---------+---------+--------------+-----------+-----------+-----------+-----------+----------+----------+-------------------+----------+-------+-------+---------+---------+-------------+\n",
      "|Copa do Brasil|   Vasco Da Gama RJ|      0.0|      0.0|           ABC|        6.0|        2.0|      133.0|       82.0|       9.0|       6.0|2023-05-06 00:30:00|2023-03-17|    0.0|    0.0|     DRAW|     DRAW|          8.0|\n",
      "|Copa do Brasil|             Gremio|      3.0|      0.0|   Ferroviario|       10.0|        3.0|       98.0|       81.0|      30.0|      11.0|2023-05-06 23:00:00|2023-03-16|    3.0|   -3.0|      WON|     LOST|         13.0|\n",
      "|Copa do Brasil|                CSA|      1.0|      0.0|       Brusque|        4.0|        4.0|      105.0|      136.0|      10.0|      13.0|2023-05-06 22:00:00|2023-03-16|    1.0|   -1.0|      WON|     LOST|          8.0|\n",
      "|Copa do Brasil|        Ypiranga RS|      3.0|      1.0|    Bragantino|        7.0|       11.0|      103.0|      119.0|       7.0|      18.0|2023-05-06 00:30:00|2023-03-16|    2.0|   -2.0|      WON|     LOST|         18.0|\n",
      "|Copa do Brasil|        Botafogo RJ|      7.0|      1.0|   Brasiliense|        8.0|        5.0|      107.0|      121.0|      14.0|      15.0|2023-05-06 23:00:00|2023-03-15|    6.0|   -6.0|      WON|     LOST|         13.0|\n",
      "|Copa do Brasil|                CRB|      5.0|      0.0|   Operario MS|        6.0|        4.0|      130.0|       94.0|      18.0|       5.0|2023-05-06 22:30:00|2023-03-15|    5.0|   -5.0|      WON|     LOST|         10.0|\n",
      "|Copa do Brasil|             Ituano|      1.0|      1.0|         Ceara|        6.0|        4.0|      103.0|       99.0|      14.0|      10.0|2023-05-06 22:00:00|2023-03-15|    0.0|    0.0|     DRAW|     DRAW|         10.0|\n",
      "|Copa do Brasil|    Aguia de Maraba|      0.0|      0.0|         Goias|        1.0|       11.0|       69.0|       95.0|       9.0|      23.0|2023-05-06 22:00:00|2023-03-15|    0.0|    0.0|     DRAW|     DRAW|         12.0|\n",
      "|Copa do Brasil|Atletico Goianiense|      1.0|      1.0| Volta Redonda|        7.0|        4.0|      128.0|       88.0|      14.0|       7.0|2023-05-06 22:00:00|2023-03-15|    0.0|    0.0|     DRAW|     DRAW|         11.0|\n",
      "|Copa do Brasil|        Nova Iguacu|      5.0|      2.0|    Nova Mutum|        6.0|        3.0|      113.0|       77.0|      15.0|       7.0|2023-05-06 18:30:25|2023-03-15|    3.0|   -3.0|      WON|     LOST|          9.0|\n",
      "|Copa do Brasil|           Tombense|      1.0|      0.0|         Retro|        6.0|        6.0|      110.0|      111.0|      14.0|      11.0|2023-05-06 00:30:00|2023-03-09|    1.0|   -1.0|      WON|     LOST|         12.0|\n",
      "|Copa do Brasil|               Remo|      2.0|      1.0|      Sao Luiz|       10.0|        3.0|      122.0|       99.0|      16.0|       7.0|2023-05-06 23:00:00|2023-03-08|    1.0|   -1.0|      WON|     LOST|         13.0|\n",
      "|Copa do Brasil|           Camboriu|      0.0|      1.0|      EC Bahia|        2.0|        6.0|       91.0|      120.0|      11.0|       8.0|2023-05-06 22:00:00|2023-03-08|   -1.0|    1.0|     LOST|      WON|          8.0|\n",
      "|Copa do Brasil|  Brasil de Pelotas|      2.0|      0.0|   Ponte Preta|        4.0|        7.0|       83.0|       94.0|       5.0|      24.0|2023-05-06 23:00:00|2023-03-07|    2.0|   -2.0|      WON|     LOST|         11.0|\n",
      "|Copa do Brasil|            Sergipe|      1.0|      1.0|   Botafogo RJ|        4.0|       10.0|       96.0|      130.0|      10.0|      11.0|2023-05-06 23:00:00|2023-03-02|    0.0|    0.0|     DRAW|     DRAW|         14.0|\n",
      "|Copa do Brasil|            Maringa|      2.0|      1.0|Sampaio Correa|        8.0|        3.0|      169.0|      112.0|      21.0|       6.0|2023-05-06 23:00:00|2023-03-02|    1.0|   -1.0|      WON|     LOST|         11.0|\n",
      "|Copa do Brasil|             Iguatu|      1.0|      0.0|    America RN|        5.0|        0.0|       16.0|       17.0|       2.0|       4.0|2023-05-06 22:15:00|2023-03-02|    1.0|   -1.0|      WON|     LOST|          5.0|\n",
      "|Copa do Brasil|     Real Ariquemes|      0.0|      3.0|      Criciuma|        5.0|        7.0|       99.0|      115.0|       6.0|      12.0|2023-05-06 19:30:00|2023-03-02|   -3.0|    3.0|     LOST|      WON|         12.0|\n",
      "|Copa do Brasil|    Aguia de Maraba|      2.0|      1.0|   Botafogo PB|        5.0|        4.0|       59.0|       44.0|      12.0|       6.0|2023-05-06 19:00:00|2023-03-02|    1.0|   -1.0|      WON|     LOST|          9.0|\n",
      "|Copa do Brasil|           Caldense|      0.0|      3.0|         Ceara|       10.0|        2.0|      106.0|       82.0|      16.0|      13.0|2023-05-06 00:30:00|2023-03-02|   -3.0|    3.0|     LOST|      WON|         12.0|\n",
      "+--------------+-------------------+---------+---------+--------------+-----------+-----------+-----------+-----------+----------+----------+-------------------+----------+-------+-------+---------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.carga at 0x1255bd990>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfile = '/Users/eduardoalberto/LoadFile/BR-Football-Dataset.csv'\n",
    "nfile = '/Users/eduardoalberto/LoadFile/BR-Football-DatasetOld.csv'\n",
    "\n",
    "class carga:\n",
    "    def __init__(self,spark):\n",
    "        self.__spark = spark\n",
    "        if path.isfile(pfile):\n",
    "            df = (self.__spark.read\n",
    "                        .format(\"csv\")\n",
    "                        .option(\"delimiter\", \",\")\n",
    "                        .option(\"header\", True)\n",
    "                        .option(\"inferSchema\", True)\n",
    "                        .load(pfile))\n",
    "        else:\n",
    "                df = (self.__spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"delimiter\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .load(nfile))\n",
    "        return df.where(col(\"tournament\")==\"Copa do Brasil\").show()\n",
    "carga(spark)\n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
